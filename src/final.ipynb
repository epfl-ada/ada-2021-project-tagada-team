{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2 project: Political Pope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neccesary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lucamusumeci/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lucamusumeci/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lucamusumeci/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import bz2\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import pickle\n",
    "import statistics as stat\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import string\n",
    "from langdetect import detect # for quotes language filtering\n",
    "import seaborn as sns # for data visualisation\n",
    "import fastcluster\n",
    "\n",
    "# NLP libraries\n",
    "import nltk, spacy, sklearn\n",
    "from spacy.lang.en import English\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize # useful ?\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# PCA steps\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = \"../data\"\n",
    "df = pd.DataFrame()\n",
    "for entry in os.scandir(Path):\n",
    "    if (entry.path.endswith(\".pkl\") and entry.is_file()):\n",
    "        file = open(entry,'rb')\n",
    "        data = pickle.load(file)\n",
    "        df = df.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote</th>\n",
       "      <th>speaker</th>\n",
       "      <th>probability</th>\n",
       "      <th>second_speaker</th>\n",
       "      <th>second_probability</th>\n",
       "      <th>third_speaker</th>\n",
       "      <th>third_probability</th>\n",
       "      <th>date</th>\n",
       "      <th>nbr_occurences</th>\n",
       "      <th>phase</th>\n",
       "      <th>nbr_words</th>\n",
       "      <th>nbr_characters</th>\n",
       "      <th>nbr_speakers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>' It is not now, nor has it ever been, the gol...</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>0.5395</td>\n",
       "      <td>None</td>\n",
       "      <td>0.3128</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>0.1477</td>\n",
       "      <td>2015-10-25 14:12:35</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>73</td>\n",
       "      <td>359</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>But if you talk about why the middle class is ...</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>0.6011</td>\n",
       "      <td>None</td>\n",
       "      <td>0.3673</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>2015-10-15 10:07:01</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>44</td>\n",
       "      <td>227</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If they destroy you, who is there left?</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>0.9061</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0939</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-04-15 17:49:00</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It would make everybody in America poorer.</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>0.5619</td>\n",
       "      <td>None</td>\n",
       "      <td>0.277</td>\n",
       "      <td>Ezra Klein</td>\n",
       "      <td>0.0821</td>\n",
       "      <td>2015-07-30 01:49:15</td>\n",
       "      <td>13</td>\n",
       "      <td>E</td>\n",
       "      <td>7</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>labor is the source of all wealth,</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>0.806</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1309</td>\n",
       "      <td>Karl Marx</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>2015-10-14 20:09:23</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>7</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102988</th>\n",
       "      <td>the Gospel of Jesus Christ is the greatest jus...</td>\n",
       "      <td>Pope Francis</td>\n",
       "      <td>0.8534</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-11 15:05:20</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>33</td>\n",
       "      <td>165</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102989</th>\n",
       "      <td>from the bishop to the foreigner, from the pri...</td>\n",
       "      <td>Pope Francis</td>\n",
       "      <td>0.7182</td>\n",
       "      <td>None</td>\n",
       "      <td>0.2818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-17 16:07:41</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>46</td>\n",
       "      <td>278</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102990</th>\n",
       "      <td>`saw the son coming from afar,' that son who h...</td>\n",
       "      <td>Pope Francis</td>\n",
       "      <td>0.7372</td>\n",
       "      <td>None</td>\n",
       "      <td>0.2628</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-20 13:13:06</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>20</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102991</th>\n",
       "      <td>This prayer of the Angelus today is a little s...</td>\n",
       "      <td>Pope Francis</td>\n",
       "      <td>0.7234</td>\n",
       "      <td>None</td>\n",
       "      <td>0.2766</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-08 20:57:39</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>28</td>\n",
       "      <td>125</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102992</th>\n",
       "      <td>We are worth so much more. We live for so much...</td>\n",
       "      <td>Pope Francis</td>\n",
       "      <td>0.7821</td>\n",
       "      <td>None</td>\n",
       "      <td>0.2179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-02-28 08:15:46</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>26</td>\n",
       "      <td>112</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>434711 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    quote         speaker  \\\n",
       "0       ' It is not now, nor has it ever been, the gol...  Bernie Sanders   \n",
       "1       But if you talk about why the middle class is ...  Bernie Sanders   \n",
       "2                 If they destroy you, who is there left?  Bernie Sanders   \n",
       "3              It would make everybody in America poorer.  Bernie Sanders   \n",
       "4                      labor is the source of all wealth,  Bernie Sanders   \n",
       "...                                                   ...             ...   \n",
       "102988  the Gospel of Jesus Christ is the greatest jus...    Pope Francis   \n",
       "102989  from the bishop to the foreigner, from the pri...    Pope Francis   \n",
       "102990  `saw the son coming from afar,' that son who h...    Pope Francis   \n",
       "102991  This prayer of the Angelus today is a little s...    Pope Francis   \n",
       "102992  We are worth so much more. We live for so much...    Pope Francis   \n",
       "\n",
       "       probability second_speaker second_probability    third_speaker  \\\n",
       "0           0.5395           None             0.3128  Hillary Clinton   \n",
       "1           0.6011           None             0.3673  Hillary Clinton   \n",
       "2           0.9061           None             0.0939              NaN   \n",
       "3           0.5619           None              0.277       Ezra Klein   \n",
       "4            0.806           None             0.1309        Karl Marx   \n",
       "...            ...            ...                ...              ...   \n",
       "102988      0.8534           None             0.1466              NaN   \n",
       "102989      0.7182           None             0.2818              NaN   \n",
       "102990      0.7372           None             0.2628              NaN   \n",
       "102991      0.7234           None             0.2766              NaN   \n",
       "102992      0.7821           None             0.2179              NaN   \n",
       "\n",
       "       third_probability                 date  nbr_occurences phase  \\\n",
       "0                 0.1477  2015-10-25 14:12:35               1     E   \n",
       "1                 0.0141  2015-10-15 10:07:01               1     E   \n",
       "2                    NaN  2015-04-15 17:49:00               1     E   \n",
       "3                 0.0821  2015-07-30 01:49:15              13     E   \n",
       "4                 0.0304  2015-10-14 20:09:23               1     E   \n",
       "...                  ...                  ...             ...   ...   \n",
       "102988               NaN  2020-03-11 15:05:20               1     E   \n",
       "102989               NaN  2020-01-17 16:07:41               1     E   \n",
       "102990               NaN  2020-03-20 13:13:06               1     E   \n",
       "102991               NaN  2020-03-08 20:57:39               1     E   \n",
       "102992               NaN  2020-02-28 08:15:46               1     E   \n",
       "\n",
       "        nbr_words  nbr_characters  nbr_speakers  \n",
       "0              73             359             3  \n",
       "1              44             227             6  \n",
       "2               8              39             2  \n",
       "3               7              42             5  \n",
       "4               7              34             5  \n",
       "...           ...             ...           ...  \n",
       "102988         33             165             2  \n",
       "102989         46             278             2  \n",
       "102990         20              96             2  \n",
       "102991         28             125             2  \n",
       "102992         26             112             2  \n",
       "\n",
       "[434711 rows x 13 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step will be to change the probabilities (type = object) into float numbers to evaluate them properly later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote</th>\n",
       "      <th>speaker</th>\n",
       "      <th>probability</th>\n",
       "      <th>second_speaker</th>\n",
       "      <th>second_probability</th>\n",
       "      <th>third_speaker</th>\n",
       "      <th>third_probability</th>\n",
       "      <th>date</th>\n",
       "      <th>nbr_occurences</th>\n",
       "      <th>phase</th>\n",
       "      <th>nbr_words</th>\n",
       "      <th>nbr_characters</th>\n",
       "      <th>nbr_speakers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>' It is not now, nor has it ever been, the gol...</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>0.5395</td>\n",
       "      <td>None</td>\n",
       "      <td>0.3128</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>0.1477</td>\n",
       "      <td>2015-10-25 14:12:35</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>73</td>\n",
       "      <td>359</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>But if you talk about why the middle class is ...</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>0.6011</td>\n",
       "      <td>None</td>\n",
       "      <td>0.3673</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>2015-10-15 10:07:01</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>44</td>\n",
       "      <td>227</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If they destroy you, who is there left?</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>0.9061</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0939</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-04-15 17:49:00</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It would make everybody in America poorer.</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>0.5619</td>\n",
       "      <td>None</td>\n",
       "      <td>0.2770</td>\n",
       "      <td>Ezra Klein</td>\n",
       "      <td>0.0821</td>\n",
       "      <td>2015-07-30 01:49:15</td>\n",
       "      <td>13</td>\n",
       "      <td>E</td>\n",
       "      <td>7</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>labor is the source of all wealth,</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>0.8060</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1309</td>\n",
       "      <td>Karl Marx</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>2015-10-14 20:09:23</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>7</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102988</th>\n",
       "      <td>the Gospel of Jesus Christ is the greatest jus...</td>\n",
       "      <td>Pope Francis</td>\n",
       "      <td>0.8534</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-11 15:05:20</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>33</td>\n",
       "      <td>165</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102989</th>\n",
       "      <td>from the bishop to the foreigner, from the pri...</td>\n",
       "      <td>Pope Francis</td>\n",
       "      <td>0.7182</td>\n",
       "      <td>None</td>\n",
       "      <td>0.2818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-17 16:07:41</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>46</td>\n",
       "      <td>278</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102990</th>\n",
       "      <td>`saw the son coming from afar,' that son who h...</td>\n",
       "      <td>Pope Francis</td>\n",
       "      <td>0.7372</td>\n",
       "      <td>None</td>\n",
       "      <td>0.2628</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-20 13:13:06</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>20</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102991</th>\n",
       "      <td>This prayer of the Angelus today is a little s...</td>\n",
       "      <td>Pope Francis</td>\n",
       "      <td>0.7234</td>\n",
       "      <td>None</td>\n",
       "      <td>0.2766</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-08 20:57:39</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>28</td>\n",
       "      <td>125</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102992</th>\n",
       "      <td>We are worth so much more. We live for so much...</td>\n",
       "      <td>Pope Francis</td>\n",
       "      <td>0.7821</td>\n",
       "      <td>None</td>\n",
       "      <td>0.2179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-02-28 08:15:46</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>26</td>\n",
       "      <td>112</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>434711 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    quote         speaker  \\\n",
       "0       ' It is not now, nor has it ever been, the gol...  Bernie Sanders   \n",
       "1       But if you talk about why the middle class is ...  Bernie Sanders   \n",
       "2                 If they destroy you, who is there left?  Bernie Sanders   \n",
       "3              It would make everybody in America poorer.  Bernie Sanders   \n",
       "4                      labor is the source of all wealth,  Bernie Sanders   \n",
       "...                                                   ...             ...   \n",
       "102988  the Gospel of Jesus Christ is the greatest jus...    Pope Francis   \n",
       "102989  from the bishop to the foreigner, from the pri...    Pope Francis   \n",
       "102990  `saw the son coming from afar,' that son who h...    Pope Francis   \n",
       "102991  This prayer of the Angelus today is a little s...    Pope Francis   \n",
       "102992  We are worth so much more. We live for so much...    Pope Francis   \n",
       "\n",
       "        probability second_speaker  second_probability    third_speaker  \\\n",
       "0            0.5395           None              0.3128  Hillary Clinton   \n",
       "1            0.6011           None              0.3673  Hillary Clinton   \n",
       "2            0.9061           None              0.0939              NaN   \n",
       "3            0.5619           None              0.2770       Ezra Klein   \n",
       "4            0.8060           None              0.1309        Karl Marx   \n",
       "...             ...            ...                 ...              ...   \n",
       "102988       0.8534           None              0.1466              NaN   \n",
       "102989       0.7182           None              0.2818              NaN   \n",
       "102990       0.7372           None              0.2628              NaN   \n",
       "102991       0.7234           None              0.2766              NaN   \n",
       "102992       0.7821           None              0.2179              NaN   \n",
       "\n",
       "        third_probability                 date  nbr_occurences phase  \\\n",
       "0                  0.1477  2015-10-25 14:12:35               1     E   \n",
       "1                  0.0141  2015-10-15 10:07:01               1     E   \n",
       "2                     NaN  2015-04-15 17:49:00               1     E   \n",
       "3                  0.0821  2015-07-30 01:49:15              13     E   \n",
       "4                  0.0304  2015-10-14 20:09:23               1     E   \n",
       "...                   ...                  ...             ...   ...   \n",
       "102988                NaN  2020-03-11 15:05:20               1     E   \n",
       "102989                NaN  2020-01-17 16:07:41               1     E   \n",
       "102990                NaN  2020-03-20 13:13:06               1     E   \n",
       "102991                NaN  2020-03-08 20:57:39               1     E   \n",
       "102992                NaN  2020-02-28 08:15:46               1     E   \n",
       "\n",
       "        nbr_words  nbr_characters  nbr_speakers  \n",
       "0              73             359             3  \n",
       "1              44             227             6  \n",
       "2               8              39             2  \n",
       "3               7              42             5  \n",
       "4               7              34             5  \n",
       "...           ...             ...           ...  \n",
       "102988         33             165             2  \n",
       "102989         46             278             2  \n",
       "102990         20              96             2  \n",
       "102991         28             125             2  \n",
       "102992         26             112             2  \n",
       "\n",
       "[434711 rows x 13 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = df.copy()\n",
    "\n",
    "#modify the types of the probabilities (from Object to float)\n",
    "new_df['probability'] = new_df['probability'].astype(np.float)\n",
    "new_df['second_probability'] = new_df['second_probability'].astype(np.float)\n",
    "new_df['third_probability'] = new_df['third_probability'].astype(np.float)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Speaker probability thresholding\n",
    "\n",
    "Each quote has a probability assigned by the algorithm that indicates the certainity (or not) it was said from the given speaker. One possible way is to put a threshold of 50% below which a quote will be removed fropm dataset. Here is the logic behind a 50% threshold: if probability >= 0.5, it means we are more confident that the speaker truly said the quote than someone else said it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.drop(new_df.query('probability <= 0.5').index, inplace=True)\n",
    "new_df = new_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Non-english quotes removal\n",
    "\n",
    "Quotebank dataset is composed of quotes of different languages. In fact the machinery(find better term) craved on websites of different languages. To perform good NLP and for the sake of comparing what is comparable, we'll remove every quote that isn't considered as english. This task will be done with the NLP language detection library 'langdetect' that supports over 55 different languages.\n",
    "\n",
    "detect_langs(\"Otec matka syn.\")\n",
    "[sk:0.572770823327, pl:0.292872522702, cs:0.134356653968]\n",
    "could be used to solidy quotes selection with p(english) >= 0.8 for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "\n",
    "for n, quote_ in enumerate(new_df['quote']) :\n",
    "    value = detect(quote_)\n",
    "    if value != \"en\":\n",
    "        index.append(n)\n",
    "        \n",
    "new_df.drop(index, inplace=True)\n",
    "new_df = new_df.reset_index(drop=True)\n",
    "\n",
    "print(\"Percentage of non-english quotes that were removed :\", round((len(df)-len(new_df))/len(df)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) NLP basic pre-processing steps\n",
    "\n",
    "To handle text properly for natural language processing, a set of classical operations must be done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ed4d13800471>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 1) remove digits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pre_processed_quote'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quote'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\d+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'new_df' is not defined"
     ]
    }
   ],
   "source": [
    "#1 remove digits\n",
    "new_df['pre_processed_quote'] = new_df['quote'].astype(str).str.replace('\\d+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-846e62496452>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#2 remove punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pre_processed_quote'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pre_processed_quote'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_punctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'new_df' is not defined"
     ]
    }
   ],
   "source": [
    "#2 remove punctuation\n",
    "new_df['pre_processed_quote'] = new_df['pre_processed_quote'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-547d0c272277>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#3 remove spaces at the beginning and ending of quotes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pre_processed_quote'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pre_processed_quote'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'new_df' is not defined"
     ]
    }
   ],
   "source": [
    "#3 remove spaces at the beginning and ending of quotes\n",
    "new_df['pre_processed_quote'] = new_df['pre_processed_quote'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-1cfaa4cc2f83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#4 remove rows with empty or NaN quotes (so also the ones which only had digits &/or punctuation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pre_processed_quote'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"pre_processed_quote\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_df' is not defined"
     ]
    }
   ],
   "source": [
    "#4 remove rows with empty or NaN quotes (so also the ones which only had digits &/or punctuation)\n",
    "new_df['pre_processed_quote'].replace(\"\", np.nan, inplace=True)\n",
    "new_df.dropna(subset = [\"pre_processed_quote\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 lower the letters of the words\n",
    "new_df['pre_processed_quote'] = new_df['pre_processed_quote'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-24-6603d052bcee>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-6603d052bcee>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    new_df['tokenized_quote'] = new_df['quote'].apply(tokenization) #important for features (voc)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#6 Quotes tokenization (useful for the removal of stop words)\n",
    "new_df['tokenized_quote'] = new_df['quote'].apply(tokenization) #important for features (voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tokenized_quote'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tokenized_quote'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-3b52c4fc5c97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#7 Quotes lemmatization to simplify the NLP. Helps to focus on the meaning of the word instead of its variations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemmatized_quote'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized_quote'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tokenized_quote'"
     ]
    }
   ],
   "source": [
    "#7 Quotes lemmatization to simplify the NLP. Helps to focus on the meaning of the word instead of its variations.\n",
    "new_df['lemmatized_quote'] = new_df['tokenized_quote'].apply(lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'remove_stop_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-ea7b95822b39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pre_processed_quote'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mremove_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized_quote'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8738\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8739\u001b[0m         )\n\u001b[0;32m-> 8740\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8742\u001b[0m     def applymap(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-ea7b95822b39>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pre_processed_quote'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mremove_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized_quote'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'remove_stop_words' is not defined"
     ]
    }
   ],
   "source": [
    "#8 remove stop words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(spacy.lang.en.stop_words.STOP_WORDS)\n",
    "stop_words.extend([singer.capitalize() for singer in stop_words]) #stop words with capitale\n",
    "stop_words.extend(list(string.punctuation))\n",
    "\n",
    "new_df['pre_processed_quote'] = new_df.apply(lambda x: remove_stop_words(x['tokenized_quote'], stop_words), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extraction (data processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
